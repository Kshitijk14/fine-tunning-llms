{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Training Dataset:\n",
    "\n",
    "Link: https://huggingface.co/datasets/Open-Orca/OpenOrca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "\n",
    "* https://huggingface.co/docs/hub/datasets-usage\n",
    "* https://docs.wandb.ai/guides/integrations/huggingface\n",
    "* https://mlabonne.github.io/blog/notes/Large%20Language%20Models/orca.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\Documents\\GitHub\\fine-tunning-llms\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Open-Orca/OpenOrca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'system_prompt', 'question', 'response'],\n",
      "        num_rows: 4233923\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and Tokenize Instructions Based on Token Count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1387 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 1024  # Define the maximum sequence length\n",
    "\n",
    "def filter_and_tokenize(data):\n",
    "    filtered_data = []\n",
    "    for item in data:\n",
    "        tokens = tokenizer.tokenize(item['response'])\n",
    "        if len(tokens) >= 100:\n",
    "            # Truncate the sequence if it exceeds the maximum length\n",
    "            truncated_tokens = tokens[:MAX_LENGTH]\n",
    "            item['response'] = tokenizer.convert_tokens_to_string(truncated_tokens)\n",
    "            filtered_data.append(item)\n",
    "    return filtered_data\n",
    "\n",
    "# Filter and tokenize the dataset\n",
    "filtered_train = filter_and_tokenize(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 1271780\n",
      "Validation data size: 317945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, valid_data = train_test_split(filtered_train, test_size=0.2)\n",
    "\n",
    "print(f\"Train data size: {len(train_data)}\")\n",
    "print(f\"Validation data size: {len(valid_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Deduplication Using Cosine Similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate(data, threshold=0.95):\n",
    "    responses = [item['response'] for item in data]\n",
    "    vectorizer = CountVectorizer().fit_transform(responses)\n",
    "    vectors = csr_matrix(vectorizer)\n",
    "\n",
    "    unique_data = []\n",
    "    seen_indices = set()\n",
    "\n",
    "    for idx in range(vectors.shape[0]):\n",
    "        if idx in seen_indices:\n",
    "            continue\n",
    "\n",
    "        similarities = cosine_similarity(vectors[idx], vectors).flatten()\n",
    "        similar_indices = np.where(similarities > threshold)[0]\n",
    "\n",
    "        seen_indices.update(similar_indices)\n",
    "        unique_data.append(data[idx])\n",
    "\n",
    "    return unique_data\n",
    "\n",
    "# Deduplicate the filtered dataset\n",
    "unique_train = deduplicate(train_data)\n",
    "unique_valid = deduplicate(valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Token Distribution Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_token_distribution(data):\n",
    "    token_counts = [len(tokenizer.tokenize(item['response'])) for item in data]\n",
    "    plt.hist(token_counts, bins=50)\n",
    "    plt.xlabel('Number of Tokens')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Token Distribution')\n",
    "    plt.show()\n",
    "\n",
    "plot_token_distribution(unique_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish the Dataset on Huggingface:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Mistral Using the New Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"mistral\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistral\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['response'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating Weights and Biases for Monitoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushing the Merged Model on Huggingface:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
